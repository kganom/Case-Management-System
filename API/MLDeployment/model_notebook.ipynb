{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import os\n",
    "path=\"C:/Users/KganoM/Downloads/doc\"    # set your local directory\n",
    "os.chdir(path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "cases_table = pd.read_excel(\"Cases_Table.xlsx\")\n",
    "clients_table = pd.read_excel(\"Clients_Table.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "# merge and sort data\n",
    "\n",
    "df = pd.merge(cases_table, clients_table, on='client_ID', how='inner')\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['resolution_time'] = pd.to_datetime(df['resolution_time'], format='%d-%m-%Y %H:%M:%S')\n",
    "df = df.sort_values(by='resolution_time')   # sort by date in order to perform a time-based train-test split \n",
    "\n",
    "df.shape   # view dimensions of dataset\n",
    "df.head()  # preview the dataset\n",
    "df.info()  # view summary of the dataset\n",
    "\n",
    "# handle missing values\n",
    "\n",
    "df.isnull().sum()    # check for missing values in predivction input varibales\n",
    "# -- prediction input variables: case_type, age, risk_level, previous_cases\n",
    "# -- none of the inputs have missing values and therefore no tasks is required to exclude or impute missing values\n",
    "\n",
    "# categorical encoding\n",
    "\n",
    "df['case_type_dummy'] = np.where(df['case_type']=='civil', 1, 0)       # create dummy variable for categorical inputs. A 1/0 is encoding is beeter is you jhave less categorical lists\n",
    "df['previous_cases_dummy'] = np.where(df['previous_cases']=='Y', 1, 0)\n",
    "\n",
    "df.to_excel('./API/MLDeployment/data/model_data.xlsx', sheet_name='Sheet1', index=False)  # export detail model data\n",
    "\n",
    "# normalize/scale features \n",
    "# -- this technique is not required for decision tree models. It is commonly used when training a neural network for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Selection\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)\n",
    "train_set, test_set= np.split(df, [int(.70 *len(df))])  # That makes the train_set with the first 70% of the data, and the test_set with rest 30% of the data.\n",
    "\n",
    "X_train = train_set.loc[:, ['case_type_dummy','age','risk_level','previous_cases_dummy']]\n",
    "X_test = test_set.loc[:, ['case_type_dummy','age','risk_level','previous_cases_dummy']]\n",
    "\n",
    "y_train = train_set['outcome']\n",
    "y_test = test_set['outcome']\n",
    "\n",
    "X_train.shape, X_test.shape  # check shape of splitted data\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0) \n",
    "model.fit(X_train, y_train)  \n",
    "\n",
    "y_train_pred = model.predict(X_train) # prediction model on train set\n",
    "\n",
    "y_test_pred = model.predict(X_test)   # prediction model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Evaluation\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred, pos_label='positive', average='micro')\n",
    "recall_train = recall_score(y_train, y_train_pred, pos_label='positive', average='micro')\n",
    "f1_train = f1_score(y_train, y_train_pred, pos_label='positive', average='micro')\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred, pos_label='positive', average='micro')\n",
    "recall_test = recall_score(y_test, y_test_pred, pos_label='positive', average='micro')\n",
    "f1_test = f1_score(y_test, y_test_pred, pos_label='positive', average='micro')\n",
    "\n",
    "print('Train:', accuracy_train)\n",
    "print('Train:', precision_train)\n",
    "print('Train:', recall_train)\n",
    "print('Train:', f1_train)\n",
    "\n",
    "print('Test:', accuracy_test)\n",
    "print('Test:', precision_test)\n",
    "print('Test:', recall_test)\n",
    "print('Test:', f1_test)\n",
    "\n",
    "# -- alignment in model performance between train and test sets is an indication of model with better Generalization\n",
    "\n",
    "dump(model, './API/MLDeployment/data/model.joblib')  # save the model for deployment as an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Importance Analysis\n",
    "\n",
    "# -- Decision trees, such as Classification and Regression Trees (CART), \n",
    "# -- calculate feature importance based on the reduction in a criterion (e.g., Gini impurity or entropy) used to select split points\n",
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "for i, v in enumerate(importance):\n",
    "    print(f'Feature: {i}, Score: {v:.5f}')\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy the model as an API using Flask or FastAPI. \n",
    "\n",
    "# -- open vs code\n",
    "# -- cd C:\\Users\\KganoM\\Downloads\\doc\\API\n",
    "# -- organize app.py file with python method and index.html file for ML model deployment as an API\n",
    "# -- pip install virtualenv\n",
    "# -- python -m venv myenv\n",
    "# -- Set-ExecutionPolicy Unrestricted -Scope Process  (optional)\n",
    "# -- myenv\\Scripts\\activate\n",
    "# -- pip install flask, pandas, scikit-learn\n",
    "# -- cd MLDeployment\n",
    "# -- Run the project using 'python app.py' and navigate to 127.0.0.1:5000 in your browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
